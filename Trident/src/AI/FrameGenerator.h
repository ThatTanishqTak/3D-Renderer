#pragma once

#include "AI/ONNXRuntime.h"

#include <array>
#include <chrono>
#include <condition_variable>
#include <cstdint>
#include <mutex>
#include <optional>
#include <span>
#include <stop_token>
#include <thread>
#include <memory>
#include <vector>

#include <vulkan/vulkan.h>

namespace Trident
{
    namespace AI
    {
        /**
         * @brief Encapsulates the metadata required to identify the GPU resources consumed by a frame.
         *
         * The descriptors are captured on the render thread and then transferred to the background
         * inference worker. The FrameGenerator never dereferences the handles directly which makes it
         * safe to enqueue work even when the GPU ownership model changes in the future.
         */
        struct FrameDescriptors
        {
            struct CPUBufferView
            {
                VkBuffer m_Buffer = VK_NULL_HANDLE;        ///< Buffer storing CPU-visible staging pixels.
                VkDeviceSize m_Offset = 0;                  ///< Byte offset where the relevant image begins.
                VkDeviceSize m_Size = 0;                    ///< Total number of bytes copied for the image.
                VkFormat m_Format = VK_FORMAT_UNDEFINED;    ///< Format used when the GPU produced the pixels.
                VkExtent2D m_Extent{ 0, 0 };                ///< Dimensions of the source image.
                uint32_t m_RowPitch = 0;                    ///< Number of bytes that make up one scanline in the staging buffer.
                void* m_MappedMemory = nullptr;             ///< Persistently mapped pointer for CPU consumers.
            };

            VkDescriptorImageInfo m_Colour{}; ///< Descriptor describing the colour buffer for the frame.
            VkDescriptorImageInfo m_Depth{}; ///< Descriptor describing the depth buffer for the frame.
            VkDescriptorImageInfo m_Motion{}; ///< Descriptor describing the motion-vector buffer for the frame.
            CPUBufferView m_ColourReadback{}; ///< CPU-visible staging view for the colour buffer.
            CPUBufferView m_DepthReadback{}; ///< CPU-visible staging view for the depth buffer.
            CPUBufferView m_MotionReadback{}; ///< CPU-visible staging view for the motion buffer (reserved for future work).
        };

        /**
         * @brief Records timing metadata captured by the render thread.
         *
         * The enqueue timestamp allows the worker to derive queue latency while the render window lets
         * developers correlate inference timings with rendering spikes. Callers should keep the lifetime
         * of these timestamps confined to the originating thread to avoid cross-thread clock skew.
         */
        struct FrameTimingMetadata
        {
            std::chrono::steady_clock::time_point m_RenderStart{}; ///< Timestamp representing the start of the render work.
            std::chrono::steady_clock::time_point m_RenderEnd{}; ///< Timestamp representing the end of the render work.
            std::chrono::steady_clock::time_point m_EnqueueTime{}; ///< Timestamp captured immediately before enqueueing.
            double m_RenderDeltaMilliseconds = 0.0; ///< Convenience field allowing callers to store per-frame delta timing.
        };

        /**
         * @brief Stores the result generated by the inference worker.
         *
         * The output tensor is copied into CPU visible memory owned by the result structure so callers can
         * safely inspect the data after dequeuing without racing the background worker. Tooling can clone
         * or process the vector immediately knowing the FrameGenerator will reuse its internal buffers for
         * future submissions.
         */
        struct FrameInferenceResult
        {
            FrameDescriptors m_Descriptors{}; ///< Descriptors mirrored back so tooling can validate resource usage.
            FrameTimingMetadata m_Timing{}; ///< Timing metadata associated with the original submission.
            std::chrono::nanoseconds m_InferenceDuration{ 0 }; ///< Duration spent executing the ONNX session.
            std::chrono::nanoseconds m_QueueLatency{ 0 }; ///< Time spent waiting in the queue before inference started.
            std::vector<float> m_OutputTensor{}; ///< Copy of the inference output data; callers own its lifetime.
        };

        /**
         * @brief Owns an ONNX runtime instance and executes inference on a background thread.
         *
         * The generator double-buffers input and output tensors so the render thread can continue to
         * submit frames while the worker consumes the previous payload. Synchronisation is handled with
         * a mutex/condition variable pair to keep the implementation compatible with the MSVC toolchain.
         * GPU interop hooks will be layered on top of these descriptors once the renderer exposes shared
         * memory handles, allowing the current staging path to evolve without disrupting the API surface.
         */
        class FrameGenerator
        {
        public:
            FrameGenerator();
            ~FrameGenerator();

            FrameGenerator(const FrameGenerator&) = delete;
            FrameGenerator& operator=(const FrameGenerator&) = delete;
            FrameGenerator(FrameGenerator&&) = delete;
            FrameGenerator& operator=(FrameGenerator&&) = delete;

            // Pass-through configuration helper so callers can toggle CUDA before loading the model.
            void EnableCUDA(bool enableCUDA);

            // Pass-through configuration helper so callers can keep the CPU fallback active for debugging.
            void EnableCPUFallback(bool enableCPUFallback);

            // Loads the requested model and primes any tensor buffers that rely on model metadata.
            bool LoadModel(const std::string& modelPath);

            /**
             * @brief Enqueues a frame for asynchronous inference.
             *
             * @param descriptors Colour, depth, and motion-vector descriptors captured from the render thread.
             * @param timing Metadata describing the frame timing and enqueue moment.
             * @param input Linear tensor data built from the descriptors; ownership stays with the caller.
             * @param inputShape Shape describing the linear tensor so the worker can forward it to the runtime.
             *
             * The caller must ensure @p input remains alive until this method returns because the data is
             * copied into a double-buffered staging area. The method blocks if both buffers are busy which
             * keeps memory usage predictable while the worker processes the previous frame.
             */
            bool EnqueueFrame(const FrameDescriptors& descriptors, const FrameTimingMetadata& timing, std::span<const float> input, std::span<const int64_t> inputShape);

            /**
             * @brief Attempts to dequeue the most recent completed inference result.
             *
             * The call is non-blocking to keep UI or diagnostic tools responsive. When a frame is available
             * the result contains a copy of the output tensor which remains valid even after the generator
             * resumes processing future frames. If no result is ready the optional is empty.
             */
            std::optional<FrameInferenceResult> DequeueFrame();

        private:
            struct FrameBuffer
            {
                std::unique_ptr<float, void(*)(void*)> m_InputTensor{ nullptr, nullptr }; ///< CPU buffer mirroring the render data.
                size_t m_InputCapacity = 0; ///< Number of float elements allocated for the input tensor.
                size_t m_InputElementCount = 0; ///< Number of valid elements stored in the input tensor.
                std::vector<int64_t> m_InputShape{}; ///< Shape forwarded to the ONNX runtime.

                std::unique_ptr<float, void(*)(void*)> m_OutputTensor{ nullptr, nullptr }; ///< Buffer used to cache the inference result.
                size_t m_OutputCapacity = 0; ///< Allocated capacity of the output tensor buffer.
                size_t m_OutputElementCount = 0; ///< Number of elements produced by the last inference pass.

                FrameDescriptors m_Descriptors{}; ///< Snapshot of the descriptors associated with the submission.
                FrameTimingMetadata m_Timing{}; ///< Copy of the timing metadata provided by the caller.

                std::chrono::steady_clock::time_point m_EnqueueTimestamp{}; ///< Moment the buffer transitioned to pending.
                std::chrono::steady_clock::time_point m_DispatchTimestamp{}; ///< Moment the worker picked up the buffer.
                std::chrono::nanoseconds m_InferenceDuration{ 0 }; ///< Duration of the most recent inference execution.

                bool m_InputReady = false; ///< True when the render thread has populated the input buffer.
                bool m_ResultReady = false; ///< True when the worker has produced an output for this buffer.
                bool m_InferenceRunning = false; ///< True while the worker thread is executing inference.
            };

        private:
            void WorkerMain(std::stop_token stopToken);
            void EnsureInputCapacity(FrameBuffer& buffer, size_t elementCount);
            void EnsureOutputCapacity(FrameBuffer& buffer, size_t elementCount);

        private:
            AI::ONNXRuntime m_Runtime; ///< Embedded runtime instance so the generator can issue inference calls.

            std::array<FrameBuffer, 2> m_Buffers{}; ///< Double-buffered staging area shared between threads.
            std::mutex m_BufferMutex; ///< Protects access to the buffer state and queues.
            std::condition_variable m_WorkAvailable; ///< Signals the worker thread when new work has arrived.
            std::condition_variable m_ResultAvailable; ///< Signals producers when buffers or results change state.
            std::jthread m_Worker; ///< Background worker that owns the ONNX dispatch calls.
        };
    }
}